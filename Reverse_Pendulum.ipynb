{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reverse Pendulum.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kovika98/reverse_pendulum/blob/dev/Reverse_Pendulum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk8hL_tH28ZZ"
      },
      "source": [
        "!rm -rf ./sample_data\n",
        "\n",
        "from dataclasses import dataclass, replace as dt_replace\n",
        "import numpy as np\n",
        "from copy import copy\n",
        "import gym\n",
        "from gym import spaces\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YhOUU1PvrYD"
      },
      "source": [
        "# Környezet implementációja :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZyoH5GM0mHM"
      },
      "source": [
        "## Dataclass - State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzCRxwBOrL6W"
      },
      "source": [
        "@dataclass\n",
        "class State:\n",
        "    p_G: float = 0.0\n",
        "    p_dG: float = 0.0\n",
        "    c_X: float = 0.0\n",
        "    c_dX: float = 0.0\n",
        "\n",
        "    def flatten(self):\n",
        "        return np.array([self.p_G, self.p_dG, self.c_X, self.c_dX])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdPGIXYwBSpj"
      },
      "source": [
        "## Env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_rZpxIw2g6r"
      },
      "source": [
        "class DoubleCartPoleEnv(gym.Env):\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return self._action_space\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        return self._observation_space\n",
        "\n",
        "    def __init__(self, timeStep=0.1):\n",
        "        # Boundaries\n",
        "        self.maxX = 2.4\n",
        "        self.maxG = 12 * 2 * np.pi / 360\n",
        "        self.maxT = 0.8\n",
        "\n",
        "        # Cart data\n",
        "        self.mC = 1\n",
        "        self.mP = 0.1\n",
        "        self.lenP = 1\n",
        "        self.radW = 0.2\n",
        "        self.friction = 1\n",
        "        self.coefR = 0.001\n",
        "\n",
        "        # Universal constants\n",
        "        self.g = 9.81\n",
        "\n",
        "        # Computed values\n",
        "        self.p_I = 1/3 * self.mP * (self.lenP ** 2)\n",
        "        self.mTot = self.mC + self.mP\n",
        "        self.dt = timeStep\n",
        "        # spaces.Box(-self.maxT, self.maxT, shape=(1,))\n",
        "        self._action_space = spaces.Discrete(7)\n",
        "        boundary = np.array([self.maxG,\n",
        "                             np.finfo(np.float32).max,\n",
        "                             self.maxX,\n",
        "                             np.finfo(np.float32).max],\n",
        "                            dtype=np.float32)\n",
        "        self._observation_space = spaces.Box(-boundary,\n",
        "                                             boundary, dtype=np.float32)\n",
        "\n",
        "        self.viewer = None\n",
        "\n",
        "    def step(self, action):\n",
        "        torque = np.linspace(-self.maxT, self.maxT,\n",
        "                              self.action_space.n)[action]\n",
        "        F = (2.0*torque - self.coefR*self.mTot*self.g/2.0)/self.radW\n",
        "\n",
        "        sinG = np.sin(self.state.p_G)\n",
        "        cosG = np.cos(self.state.p_G)\n",
        "\n",
        "        _a = (-1 * F - self.mP * 0.5 * self.lenP * (self.state.p_dG ** 2) *\n",
        "              sinG) / self.mTot\n",
        "        _b = (4/3 - self.mP * (cosG ** 2) / self.mTot)\n",
        "\n",
        "        p_ddG = (self.g * sinG + cosG * _a) \\\n",
        "            / (0.5 * self.lenP * _b)\n",
        "\n",
        "        _c = (self.state.p_dG ** 2) * sinG - \\\n",
        "            p_ddG * cosG\n",
        "        c_ddX = (F + self.mP * 0.5 * self.lenP * _c) / self.mTot\n",
        "\n",
        "        self.state.c_dX += self.dt * c_ddX\n",
        "        self.state.c_X += self.dt * self.state.c_dX\n",
        "\n",
        "        self.state.p_dG += self.dt * p_ddG\n",
        "        self.state.p_G += self.dt * self.state.p_dG\n",
        "\n",
        "        terminate = False\n",
        "        if np.abs(self.state.p_G) > self.maxG or np.abs(self.state.c_X) > \\\n",
        "          self.maxX:\n",
        "            terminate = True\n",
        "        return np.array(dt_replace(self.state).flatten()), 1.0, terminate, \\\n",
        "          {\"action\": action}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = State(p_dG=0.01)\n",
        "        return np.array(dt_replace(self.state).flatten())\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        screen_w = 600\n",
        "        screen_h = 400\n",
        "\n",
        "        world_width = self.maxX * 2\n",
        "        scale = screen_w/world_width\n",
        "        pole_w = 8.0\n",
        "        polelen = scale * self.lenP\n",
        "        car_w = 70.0\n",
        "        car_h = 40.0\n",
        "        wheel_r = 8.0\n",
        "\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(screen_w, screen_h)\n",
        "\n",
        "            # Wheel geometry\n",
        "            wheel = rendering.make_circle(wheel_r)\n",
        "            self.wheeltrans_l = rendering.Transform(\n",
        "                translation=(-1/3*car_w, wheel_r))\n",
        "            wheel.set_color(.6, .6, .6)\n",
        "            wheel.add_attr(self.wheeltrans_l)\n",
        "            self.viewer.add_geom(wheel)\n",
        "            wheel = rendering.make_circle(wheel_r)\n",
        "            self.wheeltrans_r = rendering.Transform(\n",
        "                translation=(1/3*car_w, wheel_r))\n",
        "            wheel.set_color(.6, .6, .6)\n",
        "            wheel.add_attr(self.wheeltrans_r)\n",
        "            self.viewer.add_geom(wheel)\n",
        "\n",
        "            # Car geometry\n",
        "            l, r, t, b = -car_w / 2, car_w / 2, car_h, 0\n",
        "            car = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "            self.cartrans = rendering.Transform(translation=(0, wheel_r))\n",
        "            car.add_attr(self.cartrans)\n",
        "            car.set_color(0.3, 0.3, 1)\n",
        "            self.viewer.add_geom(car)\n",
        "\n",
        "            # Pole geometry\n",
        "            l, r, t, b = -pole_w / 2, pole_w / 2, polelen, 0\n",
        "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "            pole.set_color(0, 0.9, 0.4)\n",
        "            self.poletrans = rendering.Transform(translation=(0, car_h))\n",
        "            pole.add_attr(self.poletrans)\n",
        "            pole.add_attr(self.cartrans)\n",
        "            self.viewer.add_geom(pole)\n",
        "\n",
        "            # Axle\n",
        "            axle = rendering.make_circle(pole_w/2)\n",
        "            axle.add_attr(self.poletrans)\n",
        "            axle.add_attr(self.cartrans)\n",
        "            axle.set_color(.45, .45, .45)\n",
        "            self.viewer.add_geom(axle)\n",
        "\n",
        "            self._pole_geom = pole\n",
        "\n",
        "        if self.state is None:\n",
        "            return None\n",
        "\n",
        "        pole = self._pole_geom\n",
        "        l, r, t, b = -pole_w / 2, pole_w / 2, polelen, 0\n",
        "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
        "\n",
        "        s = self.state\n",
        "        carX = s.c_X * scale + screen_w / 2  # middle of cart\n",
        "        self.wheeltrans_l.set_translation(carX - 1/3 * car_w, wheel_r)\n",
        "        self.wheeltrans_r.set_translation(carX + 1/3 * car_w, wheel_r)\n",
        "        self.cartrans.set_translation(carX, wheel_r)\n",
        "        self.poletrans.set_rotation(-s.p_G)\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
        "\n",
        "    def close(self):\n",
        "        if self.viewer:\n",
        "            self.viewer.close()\n",
        "            self.viewer = None"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZJ3UJLj0KL4"
      },
      "source": [
        "# Modell implementációja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NweFDFW60QOs"
      },
      "source": [
        "## Modell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWqmYrhA9TKD"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as kl\n",
        "\n",
        "class ProbabilityDistribution(tf.keras.Model):\n",
        "    def call(self, logits, **kwargs):\n",
        "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
        "\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__('mlp_policy')\n",
        "        # Note: no tf.get_variable(), just simple Keras API!\n",
        "        self.hidden1 = kl.Dense(128, activation='relu')\n",
        "        self.hidden2 = kl.Dense(128, activation='relu')\n",
        "        self.value = kl.Dense(1, name='value')\n",
        "        # Logits are unnormalized log probabilities.\n",
        "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
        "        self.dist = ProbabilityDistribution()\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # Inputs is a numpy array, convert to a tensor.\n",
        "        x = tf.convert_to_tensor(inputs)\n",
        "        # Separate hidden layers from the same input tensor.\n",
        "        hidden_logs = self.hidden1(x)\n",
        "        hidden_vals = self.hidden2(x)\n",
        "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
        "\n",
        "    def action_value(self, obs):\n",
        "        # Executes `call()` under the hood.\n",
        "        logits, value = self.predict_on_batch(obs)\n",
        "        action = self.dist.predict_on_batch(logits)\n",
        "        # Another way to sample actions:\n",
        "        #   action = tf.random.categorical(logits, 1)\n",
        "        # Will become clearer later why we don't use it.\n",
        "        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKD5lfCj0SkX"
      },
      "source": [
        "## Ágens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o00rOK1pqwa8"
      },
      "source": [
        "import tensorflow.keras.losses as kls\n",
        "import tensorflow.keras.optimizers as ko\n",
        "\n",
        "\n",
        "class A2CAgent:\n",
        "    def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n",
        "        # `gamma` is the discount factor\n",
        "        self.gamma = gamma\n",
        "        # Coefficients are used for the loss terms.\n",
        "        self.value_c = value_c\n",
        "        self.entropy_c = entropy_c\n",
        "\n",
        "        self.model = model\n",
        "        self.model.compile(\n",
        "            optimizer=ko.RMSprop(lr=lr),\n",
        "            # Define separate losses for policy logits and value estimate.\n",
        "            loss=[self._logits_loss, self._value_loss])\n",
        "\n",
        "    def test(self, env, render=True):\n",
        "        obs, done, ep_reward = env.reset(), False, 0\n",
        "        while not done:\n",
        "            if render:\n",
        "                env.render()\n",
        "            action, _ = self.model.action_value(obs[None, :])\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            ep_reward += reward\n",
        "        return ep_reward\n",
        "\n",
        "    def _value_loss(self, returns, value):\n",
        "        # Value loss is typically MSE between value estimates and returns.\n",
        "        return self.value_c * kls.mean_squared_error(returns, value)\n",
        "\n",
        "    def _logits_loss(self, actions_and_advantages, logits):\n",
        "        # A trick to input actions and advantages through the same API.\n",
        "        actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n",
        "\n",
        "        # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
        "        # `from_logits` argument ensures transformation into normalized probabilities.\n",
        "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(\n",
        "            from_logits=True)\n",
        "\n",
        "        # Policy loss is defined by policy gradients, weighted by advantages.\n",
        "        # Note: we only calculate the loss on the actions we've actually taken.\n",
        "        actions = tf.cast(actions, tf.int32)\n",
        "        policy_loss = weighted_sparse_ce(\n",
        "            actions, logits, sample_weight=advantages)\n",
        "\n",
        "        # Entropy loss can be calculated as cross-entropy over itself.\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        entropy_loss = kls.categorical_crossentropy(probs, probs)\n",
        "\n",
        "        # We want to minimize policy and maximize entropy losses.\n",
        "        # Here signs are flipped because the optimizer minimizes.\n",
        "        return policy_loss - self.entropy_c * entropy_loss\n",
        "\n",
        "    def train(self, env, batch_sz=64, updates=250):\n",
        "        # Storage helpers for a single batch of data.\n",
        "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
        "        rewards, dones, values = np.empty((3, batch_sz))\n",
        "        observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
        "\n",
        "        # Training loop: collect samples, send to optimizer, repeat updates times.\n",
        "        ep_rewards = [0.0]\n",
        "        next_obs = env.reset()\n",
        "        for update in range(updates):\n",
        "            for step in range(batch_sz):\n",
        "                observations[step] = next_obs.copy()\n",
        "                actions[step], values[step] = self.model.action_value(\n",
        "                    next_obs[None, :])\n",
        "                next_obs, rewards[step], dones[step], _ = env.step(\n",
        "                    actions[step])\n",
        "\n",
        "                ep_rewards[-1] += rewards[step]\n",
        "                if dones[step]:\n",
        "                    ep_rewards.append(0.0)\n",
        "                    next_obs = env.reset()\n",
        "\n",
        "            _, next_value = self.model.action_value(next_obs[None, :])\n",
        "\n",
        "            returns, advs = self._returns_advantages(\n",
        "                rewards, dones, values, next_value)\n",
        "            # A trick to input actions and advantages through same API.\n",
        "            acts_and_advs = np.concatenate(\n",
        "                [actions[:, None], advs[:, None]], axis=-1)\n",
        "\n",
        "            # Performs a full training step on the collected batch.\n",
        "            # Note: no need to mess around with gradients, Keras API handles it.\n",
        "            losses = self.model.train_on_batch(\n",
        "                observations, [acts_and_advs, returns])\n",
        "\n",
        "        return ep_rewards\n",
        "\n",
        "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
        "        # `next_value` is the bootstrap value estimate of the future state (critic).\n",
        "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
        "\n",
        "        # Returns are calculated as discounted sum of future rewards.\n",
        "        for t in reversed(range(rewards.shape[0])):\n",
        "            returns[t] = rewards[t] + self.gamma * \\\n",
        "                returns[t + 1] * (1 - dones[t])\n",
        "        returns = returns[:-1]\n",
        "\n",
        "        # Advantages are equal to returns - baseline (value estimates in our case).\n",
        "        advantages = returns - values\n",
        "\n",
        "        return returns, advantages"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxU1GFfX0a_L"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtL7JMcdq1AO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75bda51a-9ae9-428c-c8d9-83ede600b959"
      },
      "source": [
        "env = DoubleCartPoleEnv()\n",
        "model = Model(num_actions=env.action_space.n)\n",
        "agent = A2CAgent(model)\n",
        "rewards_history = agent.train(env)\n",
        "print(\"Finished training, testing...\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished training, testing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlqLkdsq7CGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657e0684-5eef-44e7-e7bb-4c683a15bc84"
      },
      "source": [
        "for i in range(5):\n",
        "    print(\"Alive for %.2f seconds\" % (agent.test(env, False)/10.0))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alive for 44.60 seconds\n",
            "Alive for 35.80 seconds\n",
            "Alive for 123.90 seconds\n",
            "Alive for 13.30 seconds\n",
            "Alive for 13.00 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}