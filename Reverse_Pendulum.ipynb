{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reverse Pendulum.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kovika98/reverse_pendulum/blob/dev/Reverse_Pendulum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk8hL_tH28ZZ"
      },
      "source": [
        "!rm -rf ./sample_data\n",
        "\n",
        "from dataclasses import dataclass, replace as dt_replace\n",
        "import numpy as np\n",
        "from copy import copy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YhOUU1PvrYD"
      },
      "source": [
        "# Környezet implementációja :)\n",
        "## dataclassok\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzCRxwBOrL6W"
      },
      "source": [
        "@dataclass\n",
        "class State:\n",
        "  p_G: float = 0.0\n",
        "  p_dG: float = 0.0\n",
        "  c_X: float = 0.0\n",
        "  c_dX: float = 0.0\n",
        "\n",
        "  def flatten(self):\n",
        "    return np.array([self.p_G, self.p_dG, self.c_X, self.c_dX])\n",
        "\n",
        "@dataclass\n",
        "class Action:\n",
        "  torq: float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdPGIXYwBSpj"
      },
      "source": [
        "## Env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_rZpxIw2g6r"
      },
      "source": [
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "\n",
        "class DoubleCartPoleEnv(gym.Env):\n",
        "  @property\n",
        "  def action_space(self):\n",
        "    return self._action_space\n",
        "\n",
        "  @property\n",
        "  def observation_space(self):\n",
        "    return self._observation_space\n",
        "\n",
        "  def __init__(self, timeStep = 0.1):\n",
        "    # Boundaries\n",
        "    self.maxX = 3\n",
        "    self.maxG = 0.8\n",
        "    self.maxT = 1\n",
        "\n",
        "    # Cart data\n",
        "    self.mC = 1\n",
        "    self.mP = 0.1\n",
        "    self.lenP = 0.8\n",
        "    self.radW = 0.2\n",
        "    self.friction = 1\n",
        "    self.coefR = 0.001\n",
        "\n",
        "    # Universal constants\n",
        "    self.g = 9.81\n",
        "\n",
        "    # Computed values\n",
        "    self.p_I = 1/3 * self.mP * (self.lenP ** 2)\n",
        "    self.mTot = self.mC + self.mP\n",
        "    self.dt = timeStep\n",
        "    self._action_space = spaces.Discrete(7)#spaces.Box(-self.maxT, self.maxT, shape = (1,))\n",
        "    boundary = np.array([self.maxG*2,\n",
        "                         np.finfo(np.float32).max,\n",
        "                         self.maxX*2,\n",
        "                         np.finfo(np.float32).max],\n",
        "                        dtype=np.float32)\n",
        "    self._observation_space = spaces.Box(-boundary, boundary, dtype=np.float32)\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, action):\n",
        "    tourque = np.linspace(-self.maxT, self.maxT, self.action_space.n)[action]\n",
        "    F = (2.0*tourque - self.coefR*self.mTot*self.g/2.0)/self.radW\n",
        "\n",
        "    _a = ( -1 * F - self.mP * 0.5 * self.lenP * (self.state.p_dG ** 2) * \\\n",
        "          np.sin(self.state.p_G)) / self.mTot\n",
        "    _b = (4/3 - self.mP * (np.cos(self.state.p_G) ** 2) / self.mTot)\n",
        "\n",
        "    p_ddG = (self.g * np.sin(self.state.p_G) + np.cos(self.state.p_G) * _a) \\\n",
        "          / (0.5 * self.lenP * _b)\n",
        "\n",
        "    _c = (self.state.p_dG ** 2) * np.sin(self.state.p_G) - \\\n",
        "    p_ddG * np.cos(self.state.p_G)\n",
        "    c_ddX = (F + self.mP * 0.5 * self.lenP * _c) / self.mTot\n",
        "\n",
        "    self.state.c_dX += self.dt * c_ddX\n",
        "    self.state.c_X += self.dt * self.state.c_dX\n",
        "\n",
        "    self.state.p_dG += self.dt * p_ddG\n",
        "    self.state.p_G += self.dt * self.state.p_dG\n",
        "\n",
        "    terminate = False\n",
        "    if np.abs(self.state.p_G) > self.maxG or np.abs(self.state.c_X) > self.maxX:\n",
        "      terminate = True\n",
        "    return np.array(dt_replace(self.state).flatten()), action, 1.0, terminate\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = State(p_dG = 0.01)\n",
        "    return np.array(dt_replace(self.state).flatten())\n",
        "\n",
        "  def render(self, mode='human'):\n",
        "    pass\n",
        "\n",
        "  def close(self):\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWqmYrhA9TKD"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as kl\n",
        "\n",
        "class ProbabilityDistribution(tf.keras.Model):\n",
        "  def call(self, logits, **kwargs):\n",
        "    return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, num_actions):\n",
        "    super().__init__('mlp_policy')\n",
        "    # Note: no tf.get_variable(), just simple Keras API!\n",
        "    self.hidden1 = kl.Dense(128, activation='relu')\n",
        "    self.hidden2 = kl.Dense(128, activation='relu')\n",
        "    self.value = kl.Dense(1, name='value')\n",
        "    # Logits are unnormalized log probabilities.\n",
        "    self.logits = kl.Dense(num_actions, name='policy_logits')\n",
        "    self.dist = ProbabilityDistribution()\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    # Inputs is a numpy array, convert to a tensor.\n",
        "    x = tf.convert_to_tensor(inputs)\n",
        "    # Separate hidden layers from the same input tensor.\n",
        "    hidden_logs = self.hidden1(x)\n",
        "    hidden_vals = self.hidden2(x)\n",
        "    return self.logits(hidden_logs), self.value(hidden_vals)\n",
        "\n",
        "  def action_value(self, obs):\n",
        "    # Executes `call()` under the hood.\n",
        "    logits, value = self.predict_on_batch(obs)\n",
        "    action = self.dist.predict_on_batch(logits)\n",
        "    # Another way to sample actions:\n",
        "    #   action = tf.random.categorical(logits, 1)\n",
        "    # Will become clearer later why we don't use it.\n",
        "    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o00rOK1pqwa8"
      },
      "source": [
        "import tensorflow.keras.losses as kls\n",
        "import tensorflow.keras.optimizers as ko\n",
        "\n",
        "\n",
        "class A2CAgent:\n",
        "  def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n",
        "    # `gamma` is the discount factor\n",
        "    self.gamma = gamma\n",
        "    # Coefficients are used for the loss terms.\n",
        "    self.value_c = value_c\n",
        "    self.entropy_c = entropy_c\n",
        "\n",
        "    self.model = model\n",
        "    self.model.compile(\n",
        "      optimizer=ko.RMSprop(lr=lr),\n",
        "      # Define separate losses for policy logits and value estimate.\n",
        "      loss=[self._logits_loss, self._value_loss])\n",
        "\n",
        "  def test(self, env, render=True):\n",
        "    obs, done, ep_reward = env.reset(), False, 0\n",
        "    while not done:\n",
        "      action, _ = self.model.action_value(obs[None, :])\n",
        "      obs, _, reward, done = env.step(action)\n",
        "      ep_reward += reward\n",
        "      if render:\n",
        "        env.render()\n",
        "    return ep_reward\n",
        "\n",
        "  def _value_loss(self, returns, value):\n",
        "    # Value loss is typically MSE between value estimates and returns.\n",
        "    return self.value_c * kls.mean_squared_error(returns, value)\n",
        "\n",
        "  def _logits_loss(self, actions_and_advantages, logits):\n",
        "    # A trick to input actions and advantages through the same API.\n",
        "    actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n",
        "\n",
        "    # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
        "    # `from_logits` argument ensures transformation into normalized probabilities.\n",
        "    weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # Policy loss is defined by policy gradients, weighted by advantages.\n",
        "    # Note: we only calculate the loss on the actions we've actually taken.\n",
        "    actions = tf.cast(actions, tf.int32)\n",
        "    policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
        "\n",
        "    # Entropy loss can be calculated as cross-entropy over itself.\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    entropy_loss = kls.categorical_crossentropy(probs, probs)\n",
        "\n",
        "    # We want to minimize policy and maximize entropy losses.\n",
        "    # Here signs are flipped because the optimizer minimizes.\n",
        "    return policy_loss - self.entropy_c * entropy_loss\n",
        "\n",
        "  def train(self, env, batch_sz=64, updates=250):\n",
        "    # Storage helpers for a single batch of data.\n",
        "    actions = np.empty((batch_sz,), dtype=np.int32)\n",
        "    rewards, dones, values = np.empty((3, batch_sz))\n",
        "    observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
        "\n",
        "    # Training loop: collect samples, send to optimizer, repeat updates times.\n",
        "    ep_rewards = [0.0]\n",
        "    next_obs = env.reset()\n",
        "    for update in range(updates):\n",
        "      for step in range(batch_sz):\n",
        "        observations[step] = next_obs.copy()\n",
        "        actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
        "        next_obs, _, rewards[step], dones[step] = env.step(actions[step])\n",
        "\n",
        "        ep_rewards[-1] += rewards[step]\n",
        "        if dones[step]:\n",
        "          ep_rewards.append(0.0)\n",
        "          next_obs = env.reset()\n",
        "\n",
        "      _, next_value = self.model.action_value(next_obs[None, :])\n",
        "\n",
        "      returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
        "      # A trick to input actions and advantages through same API.\n",
        "      acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
        "\n",
        "      # Performs a full training step on the collected batch.\n",
        "      # Note: no need to mess around with gradients, Keras API handles it.\n",
        "      losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
        "\n",
        "    return ep_rewards\n",
        "\n",
        "  def _returns_advantages(self, rewards, dones, values, next_value):\n",
        "    # `next_value` is the bootstrap value estimate of the future state (critic).\n",
        "    returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
        "\n",
        "    # Returns are calculated as discounted sum of future rewards.\n",
        "    for t in reversed(range(rewards.shape[0])):\n",
        "      returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n",
        "    returns = returns[:-1]\n",
        "\n",
        "    # Advantages are equal to returns - baseline (value estimates in our case).\n",
        "    advantages = returns - values\n",
        "\n",
        "    return returns, advantages\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtL7JMcdq1AO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550bc022-df44-45d2-fd92-2c5a48f19305"
      },
      "source": [
        "env = DoubleCartPoleEnv()\n",
        "model = Model(num_actions=env.action_space.n)\n",
        "agent = A2CAgent(model)\n",
        "rewards_history = agent.train(env)\n",
        "print(\"Finished training, testing...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished training, testing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlqLkdsq7CGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ec84b5-1c36-4470-d4d0-37b7cc35e912"
      },
      "source": [
        "print(\"%d out of 200\" % agent.test(env))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260 out of 200\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}